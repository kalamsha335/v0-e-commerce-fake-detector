{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake Product Detection - EDA & Model Training\n",
        "\n",
        "Exploratory Data Analysis and baseline model training for fake product detection in e-commerce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_curve, auc, classification_report\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample dataset\n",
        "import json\n",
        "\n",
        "with open('../dataset/sample-listings.json', 'r') as f:\n",
        "    data_raw = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(data_raw['listings'])\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "print(\"Label Distribution:\")\n",
        "print(df['is_fake'].value_counts())\n",
        "print(f\"\\nFake percentage: {df['is_fake'].mean()*100:.1f}%\")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
        "df['is_fake'].value_counts().plot(kind='bar', ax=ax, color=['green', 'red'])\n",
        "ax.set_xlabel('Label (0=Real, 1=Fake)')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Class Distribution')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Price distribution by label\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "df[df['is_fake']==0]['price'].hist(ax=ax[0], bins=30, color='green', alpha=0.7)\n",
        "ax[0].set_title('Real Products - Price Distribution')\n",
        "ax[0].set_xlabel('Price')\n",
        "\n",
        "df[df['is_fake']==1]['price'].hist(ax=ax[1], bins=30, color='red', alpha=0.7)\n",
        "ax[1].set_title('Fake Products - Price Distribution')\n",
        "ax[1].set_xlabel('Price')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rating distribution by label\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "df[df['is_fake']==0][['rating', 'review_count']].plot.scatter(ax=ax[0], s=100, alpha=0.6, color='green')\n",
        "ax[0].set_title('Real Products - Rating vs Reviews')\n",
        "ax[0].set_xlabel('Rating')\n",
        "ax[0].set_ylabel('Review Count')\n",
        "\n",
        "df[df['is_fake']==1][['rating', 'review_count']].plot.scatter(ax=ax[1], s=100, alpha=0.6, color='red')\n",
        "ax[1].set_title('Fake Products - Rating vs Reviews')\n",
        "ax[1].set_xlabel('Rating')\n",
        "ax[1].set_ylabel('Review Count')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def engineer_features(df):\n",
        "    \"\"\"Engineer features from raw listing data\"\"\"\n",
        "    X = pd.DataFrame()\n",
        "    \n",
        "    # Price features\n",
        "    X['price'] = df['price']\n",
        "    X['log_price'] = np.log1p(df['price'])\n",
        "    \n",
        "    # Rating features\n",
        "    X['rating'] = df['rating']\n",
        "    X['review_count'] = df['review_count']\n",
        "    X['log_reviews'] = np.log1p(df['review_count'])\n",
        "    \n",
        "    # Derived features\n",
        "    X['rating_review_ratio'] = X['rating'] / (X['log_reviews'] + 1)\n",
        "    X['review_score'] = df['review_count'] * df['rating']\n",
        "    X['perfect_rating_low_reviews'] = ((df['rating'] >= 4.9) & (df['review_count'] < 10)).astype(int)\n",
        "    \n",
        "    # Text features\n",
        "    X['title_length'] = df['title'].str.len()\n",
        "    X['description_length'] = df['description'].fillna('').str.len()\n",
        "    X['seller_name_length'] = df['seller'].str.len()\n",
        "    \n",
        "    # Category encoding\n",
        "    for cat in df['category'].unique():\n",
        "        X[f'cat_{cat}'] = (df['category'] == cat).astype(int)\n",
        "    \n",
        "    # Country encoding (simplified)\n",
        "    X['country_CN'] = (df['country'] == 'CN').astype(int)  # Often higher fake rate\n",
        "    X['country_US'] = (df['country'] == 'US').astype(int)\n",
        "    X['country_IN'] = (df['country'] == 'IN').astype(int)\n",
        "    \n",
        "    return X\n",
        "\n",
        "X = engineer_features(df)\n",
        "y = df['is_fake']\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Features: {X.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"\\nTraining label dist: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"Test label dist: {y_test.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train RandomForest\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"=== Performance Metrics ===\")\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
        "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "ax.set_ylabel('True Label')\n",
        "ax.set_xlabel('Predicted Label')\n",
        "ax.set_title('Confusion Matrix')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 10 Features:\")\n",
        "print(feature_importance.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "top_features = feature_importance.head(10)\n",
        "ax.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
        "ax.set_yticks(range(len(top_features)))\n",
        "ax.set_yticklabels(top_features['feature'])\n",
        "ax.set_xlabel('Importance')\n",
        "ax.set_title('Top 10 Most Important Features')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Create models directory\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "with open('../models/fake_detector_v0.1.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
